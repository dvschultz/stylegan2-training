{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TWiML SG2-ADA Workshop.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvschultz/stylegan2-training/blob/main/TWiML_SG2_ADA_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2jhIyO71ht"
      },
      "source": [
        "#TWiML StyleGAN2-ADA Workshop\n",
        "\n",
        "[**Slides accompanying this workshop**](https://docs.google.com/presentation/d/1Nv1KQW2a3KMohUDwiNb5XQnR0_nlZ_mXzLdMriklJYg/edit?usp=sharing)\n",
        "\n",
        "This notebooks was made by [Derrick Schultz](https://twitter.com/dvsch) and [Lia Coleman](https://twitter.com/Lialialiacole), and contains sample code and ideas from [Mikael Christensen](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) and [Dan Shiffman](https://www.youtube.com/watch?v=vEetoBuHj8g).\n",
        "\n",
        "Derrick Schultz: [Website](https://artificial-images.com/) • [Twitter](https://twitter.com/dvsch) • [Instagram](https://www.instagram.com/dvsmethid/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn-DWSAo7b2x"
      },
      "source": [
        "## Colab Basics\n",
        "\n",
        "Colab is a tool from Google that’s similar to Jupyter notebooks. We’ll be using this because we can get a high-powered GPU for free.\n",
        "\n",
        "Colab is divided into two types of cells: text cells and code cells. To run a code cell you can either press the play button next to the cell or click in the cell and press `Shift` + `Return`.\n",
        "\n",
        "Let’s run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65PDmQ1obIl2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yd8tfyKFclQ"
      },
      "source": [
        "To make demoing easier, we can also add some functions to show images in Colab. Run the next cell to enable that code. This will let us display images and videos directly in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCREpX4lFdzM"
      },
      "source": [
        "from IPython.display import Image, display, HTML\n",
        "import base64\n",
        "import io\n",
        "\n",
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
        "def make_img_grid(images,width=360):\n",
        "    html = []\n",
        "    for image in images:\n",
        "        with open(image, \"rb\") as img_file:\n",
        "            my_string = base64.b64encode(img_file.read())\n",
        "            img_uri = \"data:image/png;base64,\" + my_string.decode('utf8')\n",
        "        html.append('<img src=\"{}\" style=\"width:{}px;display:inline;margin:1px\"/>'.format(img_uri,str(width)))\n",
        "    return ''.join(html)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHLAxaFL8BfR"
      },
      "source": [
        "## Intall the StyleGAN2-ADA repo \n",
        "\n",
        "**Only run the next cell once per session.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vqpvoqZSIh"
      },
      "source": [
        "!git clone https://github.com/dvschultz/stylegan2-ada\n",
        "!pip install opensimplex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMN07Byna_7P"
      },
      "source": [
        "%cd /content/stylegan2-ada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMZDvIjK8JwS"
      },
      "source": [
        "### Download a trained model from Google Drive\n",
        "For this workshop we’ll be using NVIDIA’s MetFaces, a model trained on painted portraits from The Met."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ2zdnsfe49D"
      },
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl -O /content/network.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjceofdkQeIs"
      },
      "source": [
        "There are now many trained models available. I recommend checking out [this github repo](https://github.com/justinpinkney/awesome-pretrained-stylegan2) for available models.\n",
        "\n",
        "Files can be downloaded and dragged into the Files interface to upload to Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_rEEbkz8OA7"
      },
      "source": [
        "## Generating Images\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDYBQIEKOE6P"
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQGheIa8-x7"
      },
      "source": [
        "##Options\n",
        "`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "`--trunc`: This sets the truncation amount.\n",
        "\n",
        "`--outdir`:  Where to save the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrcWn-auDc-p"
      },
      "source": [
        "!python generate.py generate-images --network=\"/content/network.pkl\" --seeds=0-25 --outdir=\"./out/\" --trunc=0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1a0XwJDFQSK"
      },
      "source": [
        "Let’s look at a handful of images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JYzOj7lE8jp"
      },
      "source": [
        "listOfImageNames = ['/content/stylegan2-ada/out/seed0000.png',\n",
        "                    '/content/stylegan2-ada/out/seed0001.png',\n",
        "                    '/content/stylegan2-ada/out/seed0010.png',\n",
        "                    '/content/stylegan2-ada/out/seed0020.png',\n",
        "                    '/content/stylegan2-ada/out/seed0025.png',\n",
        "                ]\n",
        "\n",
        "display(HTML(make_img_grid(listOfImageNames, 200)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5PQToKaFKLb"
      },
      "source": [
        "Let’s zip the generated files and download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujFn7f2KFFYr"
      },
      "source": [
        "!zip -r generated-out.zip /content/stylegan2-ada/out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89yl0wdgcmj"
      },
      "source": [
        "Note: The above command will output it to a directory called `out`. If you already have stuff in `out`, make sure to clear it out. To delete a entire folder in Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA64kQmRgbva"
      },
      "source": [
        "!rm -r \"./out/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2_2g9M8mR-"
      },
      "source": [
        "## Truncation Traversal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJrhwew-87qt"
      },
      "source": [
        "\n",
        "Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "###Options \n",
        "`--network`: Again, point this to your .pkl file.\n",
        "\n",
        "`--seed`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kwp4LF0NeGd"
      },
      "source": [
        "!python generate.py truncation-traversal --network=\"/content/network.pkl\" --seed=0 --start=-2.0 --stop=2.0 --increment=0.1 --outdir=\"./tt\" --fps=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgTYPRYR8SJx"
      },
      "source": [
        "show_local_mp4_video(\"/content/stylegan2-ada/tt/truncation-traversal-seed1-start-2.0-stop2.0.mp4\", width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zNLI3R0hgFe"
      },
      "source": [
        "There should now be a video called something like: `truncation-traversal-seed1-start-2.0-stop2.0.mp4` in the `tt` folder. You can download just the video by right clicking on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cH9kpJOdRP1"
      },
      "source": [
        "## Interpolations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZqiyFX81SR"
      },
      "source": [
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at two different examples of interpolation: a linear interpolation and a random noise loop.\n",
        "\n",
        "Both methods require the following options:\n",
        "\n",
        "`--network`\n",
        "\n",
        "`--walk-type`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
        "\n",
        "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`--trunc`: truncation value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflWRWSF9SAd"
      },
      "source": [
        "### Linear Interpolation in Z Space\n",
        "\n",
        "Linear interpolation generates a linear path from one seed to another. The makers of StyleGAN say that doing this in the w space produces the best disentangled interpolations. But let’s start by looking at it in z space.\n",
        "\n",
        "`--seeds`: Use images you generated to control the interpolation points. If your first and last seed are the same this will produce a loop (nice for Instagram and gifs!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KVRdr0Un-Xl"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-z\" --seeds=0,2,5,10,,20,0 --outdir=\"./z-walk\" --frames 1440"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbkpFvm1JhsB"
      },
      "source": [
        "show_local_mp4_video(\"/content/stylegan2-ada/z-walk/walk-z-line0-2-5-0-24fps.mp4\", width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnaeu4Qw9Pm1"
      },
      "source": [
        "### Linear Interpolation in W Space\n",
        "\n",
        "Next let’s look at linear interpolation in w space. To do this we set `--walk-type` to `line-w`.\n",
        "\n",
        "I recommend using the exact same seeds so you see the difference. It’s often very subtle but it is different.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9PFflGACktY"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-w\" --seeds=0,2,5,0 --outdir=\"./w-walk\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7DKYVaJJqJC"
      },
      "source": [
        "show_local_mp4_video(\"/content/stylegan2-ada/w-walk/walk-w-line0-2-5-0-24fps.mp4\", width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51FJ7fk9bsl"
      },
      "source": [
        "### Noise Loop Interpolation\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--start_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbap7Trm22Wm"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"noiseloop\" --start_seed=0 --outdir=\"./noise1\" --diameter=2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yRlGe8yMSCG"
      },
      "source": [
        "show_local_mp4_video(\"/content/stylegan2-ada/noise1/walk-z-noiseloop-seed0-24fps.mp4\", width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YSz50IPdA8S"
      },
      "source": [
        "## Style Mixing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIH2vJY9t4d"
      },
      "source": [
        "Since seeds are just points in a vector space, we can do math things to them, like adding them together. You could do this thru linear interpolation and using the middle frame, but if you want to visualize a number of options here’s a simple script to do it. This takes a number of seeds to produce a grid showing what happens when you add the row and column together (this will make more sense after running it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGmdhswIOfwG"
      },
      "source": [
        "!python style_mixing.py --outdir=\"./stylemix\" --rows=85,100,75,458,1500 --cols=55,821,1789,293 --network=\"/content/network.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud6T2DDvHT_q"
      },
      "source": [
        "## Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlh4-drg90VV"
      },
      "source": [
        "Projection is the process of taking an image from outside the model and finding its nearest representation inside the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUJauM7UKJsS"
      },
      "source": [
        "First, lets make some folders to upload images to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO76JgwvE4b_"
      },
      "source": [
        "!mkdir uploads; mkdir aligned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezoKhE2VKRbW"
      },
      "source": [
        "Next, upload an image to the `uploads` folder. I recommend finding a portrait at a decent resolution (~1500px square)\n",
        "\n",
        "Then we need to make sure the photo we uploaded is aligned similarly to the dataset. Run the next cell to create an aligned image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXjhZ1BkEtEs"
      },
      "source": [
        "!python align_faces.py ./uploads ./aligned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8u7ZEuKKsgO"
      },
      "source": [
        "Now we’re ready to process the projection.\n",
        "\n",
        "### Options \n",
        "`--target`: upload an image that you want to project into the model, and then fill the `--target` option with the path to it. This can be any image, as long as it has the same aspect ratio as the dataset that the network was trained on.\n",
        "\n",
        "`--network`: your model. Just for this example, we're using the FFHQ network in the `--network` option because it's really easy to see what projection is doing when its projection on faces. But of course, you can input your own model as well!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38j4dj3eHWYq"
      },
      "source": [
        "!python projector.py --outdir=\"./projection\" --target=/content/stylegan2-ada/aligned/br_01.png --network=\"/content/network.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ibR9qRZP5fA"
      },
      "source": [
        "show_local_mp4_video(\"/content/stylegan2-ada/projection/proj.mp4\", width=1024, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwsjWIwcyUM"
      },
      "source": [
        "## Fun: Flesh Digressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zaDq47Fm8PP"
      },
      "source": [
        "[Flesh Digressions](https://aydao.ai/artwork/2020/01/17/fleshdigressions.html) is a technique from [aydao](https://twitter.com/aydaogman). This command will output a flesh digressions video called `circular-25-10-2020-09-00-34-PM.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrTLOxHcc0WZ"
      },
      "source": [
        "!python aydao_flesh_digressions.py --pkl \"/content/network.pkl\" --psi=0.6 --radius_small=40.0 --radius_large=300.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S84ETmHyQOzF"
      },
      "source": [
        "show_local_mp4_video(\"./circular-27-10-2020-10-52-03-PM.mp4\", width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR3YYt91SSw-"
      },
      "source": [
        "## More to make with StyleGAN2\n",
        "\n",
        "###Training Your Own Models\n",
        "Lia and I just finished up a class on custom training StyleGAN2-ADA models. The videos will probably be up on [YouTube](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA) in a few weeks.\n",
        "\n",
        "We have [a notebook](https://github.com/dvschultz/stylegan2-training/blob/main/Stylegan2_ada_Custom_Training.ipynb) and [a tutorial](https://www.youtube.com/watch?v=KxXqCsl-z2k) available now.\n",
        "\n",
        "###Advanced Image and Video Generation\n",
        "\n",
        "* [Audio Reaction](https://github.com/dvschultz/stylegan2-training/blob/main/StyleGAN2_ADA_AudioReactive_Pitch.ipynb)\n",
        "* [GANspace](https://github.com/dvschultz/Make-ML-Art-with-Google-Colab/blob/master/Ganspace_S2DD.ipynb)\n",
        "* [Network Bending](https://github.com/dvschultz/ml-art-colabs/blob/master/Network_Bending_Static_Images.ipynb)\n",
        "* [Network Blending](https://colab.research.google.com/drive/1tputbmA9EaXs9HL9iO21g7xN7jz_Xrko?usp=sharing)\n",
        "\n",
        "###Artificial Images Classes\n",
        "\n",
        "Recorded videos from previous classes:\n",
        "*   [StyleGAN2 In-Depth](https://www.youtube.com/playlist?list=PLWuCzxqIpJs-l4OygaHssyydjOu-AWoHv)\n",
        "*   [Making Machine Learning Art Datasets](https://www.youtube.com/playlist?list=PLWuCzxqIpJs-YLb2Yc3Y6rGJlYxVWFNoi)\n",
        "*   [Learn to Use Google Colab](https://www.youtube.com/playlist?list=PLWuCzxqIpJs8PN_bY5FON5mFqZ0I9QoSG)\n",
        "\n",
        "You can also sign up to be notified when Lia and I offer new classes here.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtce5bRgZtkW"
      },
      "source": [
        "# Now it’s your turn.\n",
        "\n",
        "Download a pre-trained pkl file and generate your own images and video."
      ]
    }
  ]
}