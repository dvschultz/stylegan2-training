{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SG2-ADA Generate.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvschultz/stylegan2-training/blob/main/SG2_ADA_Generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2jhIyO71ht"
      },
      "source": [
        "#StyleGAN2 Manipulation\n",
        "#### Inference, Truncation, Interpolation, Mixing, and Projection\n",
        "\n",
        "This notebook covers manipulating a StyleGAN2-ADA model to produce images and videos. It was made by [Derrick Schultz](https://twitter.com/dvsch) and [Lia Coleman](https://twitter.com/Lialialiacole) for their StyleGAN2 courses.\n",
        "\n",
        "Parts of this notebook contain ideas and code from [Mikael Christensen](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) and [Dan Shiffman](https://www.youtube.com/watch?v=vEetoBuHj8g)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65PDmQ1obIl2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yd8tfyKFclQ"
      },
      "source": [
        "To make demoing easier, we can also add some functions to show images in Colab. Run the next cell to enable that code.\n",
        "\n",
        "Unfortunately you can’t currently play video inside Colab, so you’ll have to download the videos to view them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCREpX4lFdzM"
      },
      "source": [
        "from IPython.display import Image, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHLAxaFL8BfR"
      },
      "source": [
        "# Setup: Download the StyleGAN2 repo \n",
        "\n",
        "This will install all the necessary libraries to use the StyleGAN2 repo. Press the play button or `shift+return` to run each cell.\n",
        "\n",
        "Only run the next cell once per session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vqpvoqZSIh"
      },
      "source": [
        "!git clone https://github.com/dvschultz/stylegan2-ada\n",
        "!pip install opensimplex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMN07Byna_7P"
      },
      "source": [
        "%cd stylegan2-ada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMZDvIjK8JwS"
      },
      "source": [
        "### Download your trained model from Google Drive\n",
        "Input the google id of your trained .pkl file. \n",
        "The -O option here is to specify the output file name and location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ2zdnsfe49D"
      },
      "source": [
        "!gdown --id 1YzZemZAp7BVW701_BZ7uabJWJJaS2g7v -O /content/network.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_rEEbkz8OA7"
      },
      "source": [
        "#Generate Images\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDYBQIEKOE6P"
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQGheIa8-x7"
      },
      "source": [
        "##Options\n",
        "`--network`\n",
        "\n",
        "Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`\n",
        "\n",
        "This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrcWn-auDc-p"
      },
      "source": [
        "!python generate.py generate-images --network=\"/content/network.pkl\" --seeds=0-10 --outdir=\"./out/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1a0XwJDFQSK"
      },
      "source": [
        "Let’s look at a handful of images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JYzOj7lE8jp"
      },
      "source": [
        "listOfImageNames = ['/content/stylegan2-ada/out/seed0000.png',\n",
        "                    '/content/stylegan2-ada/out/seed0001.png',\n",
        "                    '/content/stylegan2-ada/out/seed0002.png',\n",
        "                    '/content/stylegan2-ada/out/seed0003.png',\n",
        "                    '/content/stylegan2-ada/out/seed0004.png',\n",
        "                    ]\n",
        "\n",
        "for imageName in listOfImageNames:\n",
        "    display(Image(filename=imageName, width=400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5PQToKaFKLb"
      },
      "source": [
        "Let’s zip the generated files and download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujFn7f2KFFYr"
      },
      "source": [
        "!zip -r generated-out.zip /content/stylegan2-ada/out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWwo91FA1Cly"
      },
      "source": [
        "#### Generating Images With A Conditional model\n",
        "A conditional model is a model trained with class labels. Specify the class you want output with the `--class` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozC5O801B39"
      },
      "source": [
        "!python generate.py generate-images --network=\"/content/network.pkl\" --seeds=0-10 --outdir=\"./out/\". --class=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89yl0wdgcmj"
      },
      "source": [
        "Note: The above command will output it to a directory called `out`. If you already have stuff in `out`, make sure to clear it out. To delete a entire folder in Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA64kQmRgbva"
      },
      "source": [
        "!rm -r \"./out/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2_2g9M8mR-"
      },
      "source": [
        "# Truncation Traversal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJrhwew-87qt"
      },
      "source": [
        "\n",
        "Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "###Options \n",
        "`--network`: Again, point this to your .pkl file.\n",
        "\n",
        "`--seed`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kwp4LF0NeGd"
      },
      "source": [
        "!python generate.py truncation-traversal --network=\"/content/network.pkl\" --seed=1 --start=-2.0 --stop=2.0 --increment=0.25 --outdir=\"./tt\" --fps=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zNLI3R0hgFe"
      },
      "source": [
        "There should now be a video called something like: `truncation-traversal-seed1-start-2.0-stop2.0.mp4` in the `tt` folder. You can download just the video by right clicking on it. If you want all the individual frames that are there too, zip the `tt` folder and download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwTcU_XmFrl2"
      },
      "source": [
        "!zip -r generated-tt.zip /content/stylegan2-ada/tt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cH9kpJOdRP1"
      },
      "source": [
        "# Interpolations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZqiyFX81SR"
      },
      "source": [
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at two different examples of interpolation: a linear interpolation and a random noise loop.\n",
        "\n",
        "Both methods require the following options:\n",
        "\n",
        "`--network`: path to your .pkl file\n",
        "\n",
        "`--walk-type`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
        "\n",
        "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`--trunc`: truncation value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflWRWSF9SAd"
      },
      "source": [
        "### Linear Interpolation\n",
        "\n",
        "Linear interpolation generates a linear path from one seed to another. The makers of StyleGAN say that doing this in the w space produces the best disentangled interpolations. But let’s start by looking at it in z space.\n",
        "\n",
        "`--seeds`: Use images you generated to control the interpolation points. If your first and last seed are the same this will produce a loop (nice for Instagram and gifs!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KVRdr0Un-Xl"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-z\" --seeds=0,1,0 --outdir=\"./z-walk\" --frames=720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnaeu4Qw9Pm1"
      },
      "source": [
        "\n",
        "Next let’s look at linear interpolation in w space. To do this we set `--walk-type` to `line-w`.\n",
        "\n",
        "I recommend using the exact same seeds so you see the difference. It’s often very subtle but it is different.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9PFflGACktY"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-w\" --seeds=0,1,0 --outdir=\"./w-walk\" --frames=720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfI0AhsrAwJk"
      },
      "source": [
        "#### Spherical Linear Interpolation (slerp)\n",
        "\n",
        "This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations ad discussions.\n",
        "\n",
        "In reality I do not find a huge difference between linear and spherical interpolations (the difference in z- and w-space is enough in many cases), but I’ve implemented slerp here for anyone interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGiHawgcFYPI"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"sphere-z\" --seeds=0,1,0 --outdir=\"./z-sphere\" --frames=720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzDSRoymVRVx"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"sphere-w\" --seeds=0,1,0 --outdir=\"./w-sphere\" --frames=720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51FJ7fk9bsl"
      },
      "source": [
        "### Noise Loop Interpolation\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--start_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbap7Trm22Wm"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"noiseloop\" --start_seed=0 --outdir=\"./noise1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WAHtg_KJVpW"
      },
      "source": [
        "### Circular Loop Interpolation\n",
        "\n",
        "The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n",
        "\n",
        "I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDRmamWkJUhV"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"circularloop\" --start_seed=0 --diameter=300.0 --frames=720 --outdir=\"./circleloop1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGObczMXdCRD"
      },
      "source": [
        "# Near Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5mznFQw-FuP"
      },
      "source": [
        "**Added by popular demand**\n",
        "\n",
        "Let’s say you have a seed you like, but want to see other images like it to see if there’s something better. Now you can with the `near-neighbor` argument.\n",
        "\n",
        "### Options\n",
        "`--network`, `--seeds`, and `--truncation_psi` work the same as above.\n",
        "\n",
        "`--diameter`: this sets how far away from the seed you want to generate images. `.0000001` is really close, `.5` is reallly far.\n",
        "\n",
        "`--num_samples`: how many samples you want to produce\n",
        "\n",
        "`--save_vector`: this will save the vector as a file in the .npy format. You can then use this for interpolation (not super well supported right now, but can be used manually—see the Projection code for an example of reading a .npy file and interpolating it).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWWDQA4GOjbN"
      },
      "source": [
        "!python generate.py generate-neighbors --network=\"/content/network.pkl\" --seeds=9 --outdir=\"./neighbors\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Uq1PQTlHwL"
      },
      "source": [
        "!zip -r neighbors.zip /content/stylegan2-ada/neighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YSz50IPdA8S"
      },
      "source": [
        "# Style Mixing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIH2vJY9t4d"
      },
      "source": [
        "Since seeds are just points in a vector space, we can do math things to them, like adding them together. You could do this thru linear interpolation and using the middle frame, but if you want to visualize a number of options here’s a simple script to do it. This takes a number of seeds to produce a grid showing what happens when you add the row and column together (this will make more sense after running it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGmdhswIOfwG"
      },
      "source": [
        "!python style_mixing.py --outdir=\"./stylemix\" --rows=85,100,75,458,1500 --cols=55,821,1789,293 --network=\"/content/network.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwsjWIwcyUM"
      },
      "source": [
        "# Flesh Digressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zaDq47Fm8PP"
      },
      "source": [
        "[Flesh Digressions](https://aydao.ai/artwork/2020/01/17/fleshdigressions.html) is a technique from [aydao](https://twitter.com/aydaogman). This command will output a flesh digressions video called `circular-25-10-2020-09-00-34-PM.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrTLOxHcc0WZ"
      },
      "source": [
        "!python aydao_flesh_digressions.py --pkl \"/content/network.pkl\" --psi=0.6 --radius_small=10.0 --radius_large=800.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud6T2DDvHT_q"
      },
      "source": [
        "# Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlh4-drg90VV"
      },
      "source": [
        "Projection is the process of taking an image from outside the model and finding its nearest representation inside the model. \n",
        "\n",
        "###Options \n",
        "`--target`: upload an image that you want to project into the model, and then fill the `--target` option with the path to it. This can be any image, as long as it has the same aspect ratio as the dataset that the network was trained on.\n",
        "\n",
        "`--network`: your model. Just for this example, we're using the FFHQ network in the `--network` option because it's really easy to see what projection is doing when its projection on faces. But of course, you can input your own model as well!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38j4dj3eHWYq"
      },
      "source": [
        "!python projector.py --outdir=\"./projection\" --target=/content/stylegan2-ada/flower.jpg --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hrf1Qi-vqWG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}