{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SG2-ADA Generate.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvschultz/stylegan2-training/blob/main/SG2_ADA_Generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2jhIyO71ht"
      },
      "source": [
        "#StyleGAN2 Manipulation\n",
        "#### Inference, Truncation, Interpolation, Mixing, and Projection\n",
        "\n",
        "This notebook covers manipulating a StyleGAN2-ADA model to produce images and videos. It was made by [Derrick Schultz](https://twitter.com/dvsch) and [Lia Coleman](https://twitter.com/Lialialiacole) for their StyleGAN2 courses.\n",
        "\n",
        "Parts of this notebook contain ideas and code from [Mikael Christensen](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) and [Dan Shiffman](https://www.youtube.com/watch?v=vEetoBuHj8g)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65PDmQ1obIl2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHLAxaFL8BfR"
      },
      "source": [
        "# Setup: Download the StyleGAN2 repo \n",
        "\n",
        "This will install all the necessary libraries to use the StyleGAN2 repo. Press the play button or `shift+return` to run each cell.\n",
        "\n",
        "Only run the next cell once per session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vqpvoqZSIh"
      },
      "source": [
        "!git clone https://github.com/dvschultz/stylegan2-ada\n",
        "!pip install opensimplex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMN07Byna_7P"
      },
      "source": [
        "%cd stylegan2-ada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMZDvIjK8JwS"
      },
      "source": [
        "### Download your trained model from Google Drive\n",
        "Input the google id of your trained .pkl file. \n",
        "The -O option here is to specify the output file name and location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ2zdnsfe49D"
      },
      "source": [
        "!gdown --id 11jjGGqKJs9KpqAgKkRQde15vTjfrKOUO -O /content/network.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDYBQIEKOE6P"
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_rEEbkz8OA7"
      },
      "source": [
        "#Generate Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQGheIa8-x7"
      },
      "source": [
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model.\n",
        "The following command will generate 50 sample images from the model.\n",
        "\n",
        "##Options\n",
        "`--network`\n",
        "\n",
        "Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`\n",
        "\n",
        "This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrcWn-auDc-p"
      },
      "source": [
        "!python generate.py generate-images --network=\"/content/network.pkl\" --seeds=0-10 --outdir=\"./out/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWwo91FA1Cly"
      },
      "source": [
        "#### With conditional model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozC5O801B39"
      },
      "source": [
        "!python generate.py generate-images --network=\"/content/network.pkl\" --seeds=0-10 --outdir=\"./out/\". --class=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2_2g9M8mR-"
      },
      "source": [
        "# Truncation Traversal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJrhwew-87qt"
      },
      "source": [
        "\n",
        "Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically its infinite. \n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "###Options \n",
        "`--network`: Again, point this to your .pkl file.\n",
        "\n",
        "`--seed`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kwp4LF0NeGd"
      },
      "source": [
        "!python generate.py truncation-traversal --network=\"/content/network.pkl\" --seed=1 --start=-2.0 --stop=2.0 --increment=0.25 --outdir=\"./tt\" --fps=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cH9kpJOdRP1"
      },
      "source": [
        "# Interpolations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZqiyFX81SR"
      },
      "source": [
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at two different examples of interpolation: a linear interpolation and a random noise loop.\n",
        "\n",
        "Both methods require the following options:\n",
        "\n",
        "`--network`\n",
        "\n",
        "`--walk-type`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
        "\n",
        "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`--trunc`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflWRWSF9SAd"
      },
      "source": [
        "### Linear Interpolation\n",
        "\n",
        "Linear interpolation generates a linear path from one seed to another. The makers of StyleGAN say that doing this in the w space produces the best disentangled interpolations. But let’s start by looking at it in z space.\n",
        "\n",
        "`--seeds`: Use images you generated to control the interpolation points. If your first and last seed are the same this will produce a loop (nice for Instagram and gifs!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KVRdr0Un-Xl"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-z\" --seeds=0,2,5,0 --outdir=\"./z-walk\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnaeu4Qw9Pm1"
      },
      "source": [
        "\n",
        "Next let’s look at linear interpolation in w space. To do this we set `--walk-type` to `line-w`.\n",
        "\n",
        "I recommend using the exact same seeds so you see the difference. It’s often very subtle but it is different.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9PFflGACktY"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"line-w\" --seeds=0,2,5,0 --outdir=\"./w-walk\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51FJ7fk9bsl"
      },
      "source": [
        "### Noise Loop Interpolation\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "Seeds aren’t used here but you do need to set one for 🌟reasons🌟. Just leave it at zero but know that changing this value won’t do much.\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--start_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbap7Trm22Wm"
      },
      "source": [
        "!python generate.py generate-latent-walk --network=\"/content/network.pkl\" --walk-type=\"noiseloop\" --start_seed=0 --outdir=\"./noise1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGObczMXdCRD"
      },
      "source": [
        "# Near Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5mznFQw-FuP"
      },
      "source": [
        "**Added by popular demand**\n",
        "\n",
        "Let’s say you have a seed you like, but want to see other images like it to see if there’s something better. Now you can with the `near-neighbor` argument.\n",
        "\n",
        "### Options\n",
        "`--network`, `--seeds`, and `--truncation_psi` work the same as above.\n",
        "\n",
        "`--diameter`: this sets how far away from the seed you want to generate images. `.0000001` is really close, `.5` is reallly far.\n",
        "\n",
        "`--num_samples`: how many samples you want to produce\n",
        "\n",
        "`--save_vector`: this will save the vector as a file in the .npy format. You can then use this for interpolation (not super well supported right now, but can be used manually—see the Projection code for an example of reading a .npy file and interpolating it).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWWDQA4GOjbN"
      },
      "source": [
        "!python generate.py generate-neighbors --network=\"/content/network.pkl\" --seeds=9 --outdir=\"./neighbors\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YSz50IPdA8S"
      },
      "source": [
        "# Style Mixing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIH2vJY9t4d"
      },
      "source": [
        "Since seeds are just points in a vector space, we can do math things to them, like adding them together. You could do this thru linear interpolation and using the middle frame, but if you want to visualize a number of options here’s a simple script to do it. This takes a number of seeds to produce a grid showing what happens when you add the row and column together (this will make more sense after running it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGmdhswIOfwG"
      },
      "source": [
        "!python style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 --network=\"/content/network.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwsjWIwcyUM"
      },
      "source": [
        "# Flesh Digressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrTLOxHcc0WZ"
      },
      "source": [
        "!python aydao_flesh_digressions.py --pkl \"/content/network.pkl\" --psi=0.6 --radius_small=10.0 --radius_large=800.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud6T2DDvHT_q"
      },
      "source": [
        "# Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlh4-drg90VV"
      },
      "source": [
        "Projection is the process of taking an image from outside the model and finding its nearest representation inside the model.\n",
        "\n",
        "For projection, we’re gonna do something a little different in this notebook and load the code directly into Colab. Don’t worry if this looks scary—there’s only two things you want to edit in the code for any projection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38j4dj3eHWYq"
      },
      "source": [
        "!python projector.py --outdir=out2 --target=/content/redbull.png --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVsnHG0FPcWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}